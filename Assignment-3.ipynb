{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract training data as data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acq</td>\n",
       "      <td>COMPUTER TERMINAL SYSTEMS &lt;CPML&gt; COMPLETES SAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acq</td>\n",
       "      <td>OHIO MATTRESS &lt;OMT&gt; MAY HAVE LOWER 1ST QTR NET...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acq</td>\n",
       "      <td>MCLEAN'S &lt;MII&gt; U.S. LINES SETS ASSET TRANSFER\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acq</td>\n",
       "      <td>CHEMLAWN &lt;CHEM&gt; RISES ON HOPES FOR HIGHER BIDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acq</td>\n",
       "      <td>&lt;COFAB INC&gt; BUYS GULFEX FOR UNDISCLOSED AMOUNT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11408</th>\n",
       "      <td>zinc</td>\n",
       "      <td>PEGASUS GOLD &lt;PGULF&gt; STARTS MILLING IN MONTANA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11409</th>\n",
       "      <td>zinc</td>\n",
       "      <td>WORLD ZINC STOCKS FALL 7,700 TONNES IN FEBRUAR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11410</th>\n",
       "      <td>zinc</td>\n",
       "      <td>LME DETAILS MARCH 1987 TURNOVER\\n\\n    LONDON,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11411</th>\n",
       "      <td>zinc</td>\n",
       "      <td>BALL &lt;BLL&gt; TO SUPPLY PENNY BLANKS TO MINTS\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11412</th>\n",
       "      <td>zinc</td>\n",
       "      <td>WESTMIN TO RAISE MYRA FALLS CAPACITY BY 33 PCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11413 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class                                           Document\n",
       "0       acq  COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SAL...\n",
       "1       acq  OHIO MATTRESS <OMT> MAY HAVE LOWER 1ST QTR NET...\n",
       "2       acq  MCLEAN'S <MII> U.S. LINES SETS ASSET TRANSFER\\...\n",
       "3       acq  CHEMLAWN <CHEM> RISES ON HOPES FOR HIGHER BIDS...\n",
       "4       acq  <COFAB INC> BUYS GULFEX FOR UNDISCLOSED AMOUNT...\n",
       "...     ...                                                ...\n",
       "11408  zinc  PEGASUS GOLD <PGULF> STARTS MILLING IN MONTANA...\n",
       "11409  zinc  WORLD ZINC STOCKS FALL 7,700 TONNES IN FEBRUAR...\n",
       "11410  zinc  LME DETAILS MARCH 1987 TURNOVER\\n\\n    LONDON,...\n",
       "11411  zinc  BALL <BLL> TO SUPPLY PENNY BLANKS TO MINTS\\n\\n...\n",
       "11412  zinc  WESTMIN TO RAISE MYRA FALLS CAPACITY BY 33 PCT...\n",
       "\n",
       "[11413 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def read_documents(folder_path):\n",
    "    classes = []\n",
    "    documents = []\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "                file_path = os.path.join(root, file_name)\n",
    "                with open(file_path, 'r') as file:\n",
    "                    content = file.read()\n",
    "                    class_name = os.path.basename(root)\n",
    "                    classes.append(class_name)\n",
    "                    documents.append(content.strip())\n",
    "\n",
    "    df = pd.DataFrame({'Class': classes, 'Document': documents})\n",
    "    return df\n",
    "\n",
    "\n",
    "folder_path = './Data'\n",
    "df = read_documents(folder_path)\n",
    "\n",
    "df \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extract test data as data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './test'\n",
    "testing_df = read_documents(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove Not important  tags (\\n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Document'] = df['Document'].str.replace('\\n', ' ')\n",
    "testing_df['Document'] = testing_df['Document'].str.replace('\\n', ' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['Tokens'] = df['Document'].apply(lambda x: word_tokenize(x))\n",
    "testing_df['Tokens'] = testing_df['Document'].apply(lambda x: word_tokenize(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "df['Tokens'] = df['Tokens'].apply(lambda tokens: [porter_stemmer.stem(token) for token in tokens])\n",
    "testing_df['Tokens'] = testing_df['Tokens'].apply(lambda tokens: [porter_stemmer.stem(token) for token in tokens])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary set extraction for training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>342.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132-1/2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b.c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>clement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>automak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39517</th>\n",
       "      <td>esb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39518</th>\n",
       "      <td>qtec</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39519</th>\n",
       "      <td>120/125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39520</th>\n",
       "      <td>7,703,625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39521</th>\n",
       "      <td>18.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39522 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Tokens\n",
       "0          342.7\n",
       "1        132-1/2\n",
       "2            b.c\n",
       "3        clement\n",
       "4        automak\n",
       "...          ...\n",
       "39517        esb\n",
       "39518       qtec\n",
       "39519    120/125\n",
       "39520  7,703,625\n",
       "39521      18.62\n",
       "\n",
       "[39522 rows x 1 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_tokens = set()\n",
    "for tokens in df['Tokens']:\n",
    "    unique_tokens.update(tokens)\n",
    "\n",
    "Vocabulary  = pd.DataFrame({'Tokens': list(unique_tokens)})\n",
    "\n",
    "Vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'vocabulary.csv'\n",
    "# Vocabulary.to_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean vocabulart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile('[a-zA-Z]')\n",
    "\n",
    "filtered_tokens = [\n",
    "    [token for token in tokens if pattern.search(token) and not token.isdigit()and len(token) > 2]\n",
    "    for tokens in df['Tokens']\n",
    "]\n",
    "df['Tokens'] = filtered_tokens\n",
    "\n",
    "\n",
    "filtered_token = [\n",
    "    [token for token in tokens if pattern.search(token) and not token.isdigit()and len(token) > 2]\n",
    "    for tokens in testing_df['Tokens']\n",
    "]\n",
    "testing_df['Tokens'] = filtered_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['Document'] = df['Document'].str.replace('/', ' ')\n",
    "testing_df['Document'] = testing_df['Document'].str.replace('/', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Document</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>acq</td>\n",
       "      <td>COMPUTER TERMINAL SYSTEMS &lt;CPML&gt; COMPLETES SAL...</td>\n",
       "      <td>[comput, termin, system, cpml, complet, sale, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>acq</td>\n",
       "      <td>OHIO MATTRESS &lt;OMT&gt; MAY HAVE LOWER 1ST QTR NET...</td>\n",
       "      <td>[ohio, mattress, omt, may, have, lower, 1st, q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>acq</td>\n",
       "      <td>MCLEAN'S &lt;MII&gt; U.S. LINES SETS ASSET TRANSFER ...</td>\n",
       "      <td>[mclean, mii, u.s., line, set, asset, transfer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acq</td>\n",
       "      <td>CHEMLAWN &lt;CHEM&gt; RISES ON HOPES FOR HIGHER BIDS...</td>\n",
       "      <td>[chemlawn, chem, rise, hope, for, higher, bid,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acq</td>\n",
       "      <td>&lt;COFAB INC&gt; BUYS GULFEX FOR UNDISCLOSED AMOUNT...</td>\n",
       "      <td>[cofab, inc, buy, gulfex, for, undisclos, amou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11408</th>\n",
       "      <td>zinc</td>\n",
       "      <td>PEGASUS GOLD &lt;PGULF&gt; STARTS MILLING IN MONTANA...</td>\n",
       "      <td>[pegasu, gold, pgulf, start, mill, montana, je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11409</th>\n",
       "      <td>zinc</td>\n",
       "      <td>WORLD ZINC STOCKS FALL 7,700 TONNES IN FEBRUAR...</td>\n",
       "      <td>[world, zinc, stock, fall, tonn, februari, ein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11410</th>\n",
       "      <td>zinc</td>\n",
       "      <td>LME DETAILS MARCH 1987 TURNOVER      LONDON, A...</td>\n",
       "      <td>[lme, detail, march, turnov, london, april, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11411</th>\n",
       "      <td>zinc</td>\n",
       "      <td>BALL &lt;BLL&gt; TO SUPPLY PENNY BLANKS TO MINTS    ...</td>\n",
       "      <td>[ball, bll, suppli, penni, blank, mint, munci,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11412</th>\n",
       "      <td>zinc</td>\n",
       "      <td>WESTMIN TO RAISE MYRA FALLS CAPACITY BY 33 PCT...</td>\n",
       "      <td>[westmin, rais, myra, fall, capac, pct, calgar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11413 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Class                                           Document  \\\n",
       "0       acq  COMPUTER TERMINAL SYSTEMS <CPML> COMPLETES SAL...   \n",
       "1       acq  OHIO MATTRESS <OMT> MAY HAVE LOWER 1ST QTR NET...   \n",
       "2       acq  MCLEAN'S <MII> U.S. LINES SETS ASSET TRANSFER ...   \n",
       "3       acq  CHEMLAWN <CHEM> RISES ON HOPES FOR HIGHER BIDS...   \n",
       "4       acq  <COFAB INC> BUYS GULFEX FOR UNDISCLOSED AMOUNT...   \n",
       "...     ...                                                ...   \n",
       "11408  zinc  PEGASUS GOLD <PGULF> STARTS MILLING IN MONTANA...   \n",
       "11409  zinc  WORLD ZINC STOCKS FALL 7,700 TONNES IN FEBRUAR...   \n",
       "11410  zinc  LME DETAILS MARCH 1987 TURNOVER      LONDON, A...   \n",
       "11411  zinc  BALL <BLL> TO SUPPLY PENNY BLANKS TO MINTS    ...   \n",
       "11412  zinc  WESTMIN TO RAISE MYRA FALLS CAPACITY BY 33 PCT...   \n",
       "\n",
       "                                                  Tokens  \n",
       "0      [comput, termin, system, cpml, complet, sale, ...  \n",
       "1      [ohio, mattress, omt, may, have, lower, 1st, q...  \n",
       "2      [mclean, mii, u.s., line, set, asset, transfer...  \n",
       "3      [chemlawn, chem, rise, hope, for, higher, bid,...  \n",
       "4      [cofab, inc, buy, gulfex, for, undisclos, amou...  \n",
       "...                                                  ...  \n",
       "11408  [pegasu, gold, pgulf, start, mill, montana, je...  \n",
       "11409  [world, zinc, stock, fall, tonn, februari, ein...  \n",
       "11410  [lme, detail, march, turnov, london, april, th...  \n",
       "11411  [ball, bll, suppli, penni, blank, mint, munci,...  \n",
       "11412  [westmin, rais, myra, fall, capac, pct, calgar...  \n",
       "\n",
       "[11413 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Tokens'] = df['Tokens'].apply(' '.join)\n",
    "testing_df['Tokens'] = testing_df['Tokens'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tf_idf_vectorizer.fit(df['Tokens'])\n",
    "tf_idf_training = tf_idf_vectorizer.transform(df['Tokens'])\n",
    "tf_idf_testing = tf_idf_vectorizer.transform(testing_df['Tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "word2vec = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def document_vector(word2vec_model, doc):\n",
    "    words = [word for word in doc.split() if word in word2vec_model.key_to_index]\n",
    "    \n",
    "    if len(words) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    \n",
    "    doc_vector = np.mean(word2vec_model[words], axis=0)\n",
    "    return doc_vector\n",
    "\n",
    "training_doc_vectors = np.array([document_vector(word2vec, doc) for doc in df['Tokens']])\n",
    "testing_doc_vectors = np.array([document_vector(word2vec, doc) for doc in testing_df['Tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(training_doc_vectors)\n",
    "\n",
    "scaled_training_doc_vectors = scaler.transform(training_doc_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df['Class'].values\n",
    "y_test = testing_df['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (TF-IDF): 0.03779686385139291\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(tf_idf_training, y_train)\n",
    "\n",
    "y_pred_tfidf = nb_classifier.predict(tf_idf_testing)\n",
    "\n",
    "f1_tfidf = f1_score(y_test, y_pred_tfidf, average='macro')\n",
    "print(f'F1 Score (TF-IDF): {f1_tfidf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Word2Vec): 0.0046742535492290925\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(scaled_training_doc_vectors, y_train)\n",
    "\n",
    "y_pred_word2vec = nb_classifier.predict(testing_doc_vectors)\n",
    "\n",
    "f1_word2vec = f1_score(y_test, y_pred_word2vec, average='macro')\n",
    "print(f'F1 Score (Word2Vec): {f1_word2vec}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (TF-IDF with SVC): 0.23927578465071792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "svm_classifier = SVC()\n",
    "svm_classifier.fit(tf_idf_training, y_train)\n",
    "y_pred_tfidf_svm = svm_classifier.predict(tf_idf_testing)\n",
    "f1_tfidf_svm = f1_score(y_test, y_pred_tfidf_svm, average='macro')\n",
    "\n",
    "print(f'F1 Score (TF-IDF with SVC): {f1_tfidf_svm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Word2Vec with SVC): 0.16426533999270984\n"
     ]
    }
   ],
   "source": [
    "svm_classifier.fit(training_doc_vectors, y_train)\n",
    "\n",
    "y_pred_word2vec_svm = svm_classifier.predict(testing_doc_vectors)\n",
    "\n",
    "f1_word2vec_svm = f1_score(y_test, y_pred_word2vec_svm, average='macro')\n",
    "print(f'F1 Score (Word2Vec with SVC): {f1_word2vec_svm}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (TF-IDF with Random Forest): 0.19291925417100453\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "rf_classifier.fit(tf_idf_training, y_train)\n",
    "\n",
    "y_pred_tfidf_rf = rf_classifier.predict(tf_idf_testing)\n",
    "\n",
    "f1_tfidf_rf = f1_score(y_test, y_pred_tfidf_rf, average='macro')\n",
    "print(f'F1 Score (TF-IDF with Random Forest): {f1_tfidf_rf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Word2Vec with Random Forest): 0.1519665811367221\n"
     ]
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier()\n",
    "\n",
    "rf_classifier.fit(training_doc_vectors, y_train)\n",
    "\n",
    "y_pred_word2vec_rf = rf_classifier.predict(testing_doc_vectors)\n",
    "\n",
    "f1_word2vec_rf = f1_score(y_test, y_pred_word2vec_rf, average='macro')\n",
    "print(f'F1 Score (Word2Vec with Random Forest): {f1_word2vec_rf}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
